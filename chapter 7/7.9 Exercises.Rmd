---
title: "7.9 Exercises"
output:
  html_document:
    df_print: paged
---

# Applied

## Question 6

```{r}
library(ISLR)
library(boot)
```

### a
```{r}
set.seed(10)
cv.err = rep(0,10)
for (i in 1:10){
  polyfit = glm(wage~poly(age,i), data = Wage)
  cv.err[i] = cv.glm( Wage, polyfit, K = 10)$delta[1]
}
cv.err
```
```{r}
which.min(cv.err) #gives the degree of polynomial we need to choose
```
Clearly, 9 will be a very large degree of freedom and will result in overfitting. Let us plot the error values to see what lower value can we prefer
```{r}
plot(1:10, cv.err, type = "l")
```
We can choose 4 as the lowest possible degree of polynomial without very high CVerror
```{r}
polyfit.4 = lm(wage~poly(age,4), data = Wage)
agelims = range(Wage$age)
age.grid = seq(from = agelims[1], to = agelims[2])
polypred = predict( polyfit.4, newdata=list(age=age.grid), se=T)
polysebands = cbind( polypred$fit + 2*polypred$se.fit, polypred$fit - 2*polypred$se.fit)

plot(Wage$age, Wage$wage, col="darkgrey")
lines(age.grid, polypred$fit, col="red", lwd=2)
matlines(age.grid, polysebands, col = "red", lty = 2) #lty is line type
```
### b
```{r}
set.seed(10)
step.cv.err = rep(0,9)
for (i in 2:10){
  Wage$age.cut = cut(Wage$age, i)
  stepfit = glm(wage~age.cut, data = Wage)
  step.cv.err[i-1] = cv.glm( Wage, stepfit, K = 10)$delta[2]
}
which.min(step.cv.err)
```
```{r}
stepfit.4 = lm(wage~cut(age,7), data = Wage)
agelims = range(Wage$age)
age.grid = seq(from = agelims[1], to = agelims[2])
steppred = predict( stepfit.4, newdata=data.frame(age=age.grid), se=T)
stepsebands = cbind( steppred$fit + 2*steppred$se.fit, steppred$fit - 2*steppred$se.fit)

plot(Wage$age, Wage$wage, col="darkgrey")
lines(age.grid, steppred$fit, col="red", lwd=2)
matlines(age.grid, stepsebands, col = "red", lty = 2) #lty is line type
```

## Question 8

```{r}
pairs(Auto)
```
clearly, mpg is has a relation with cylinders, displacement, horsepower and weight
```{r}
summary(lm(mpg~cylinders+displacement+horsepower+weight, data = Auto))
```
We will try polynomial regression with displacement
```{r}
cverror = rep(0,15)
for (i in 1:10){
  poly.fit = glm( mpg~poly(displacement,i), data = Auto)
  cverror[i] = cv.glm( Auto, poly.fit, K = 10)$delta[1]
}
which.min(cverror)
```
```{r}
plot(1:15, cverror, type = "b")
```
Now, we plot the resulting polynomial fit
```{r}
poly.fit.11 = lm(mpg~poly(displacement,11), data = Auto)
dis.grid = seq( from=range(Auto$displacement)[1], to=range(Auto$displacement)[2] )
poly.pred.11 = predict( poly.fit.11, newdata=list(displacement=dis.grid), se = T)
se.bands.11 = cbind( poly.pred.11$fit+2*poly.pred.11$se.fit, poly.pred.11$fit-2*poly.pred.11$se.fit )

plot(Auto$displacement, Auto$mpg, col="black")
lines( dis.grid, poly.pred.11$fit, col="red", lwd=2)
matlines( dis.grid, se.bands.11, col="red", lty=2, lwd=1)
```
Now, we try the same with step functions
```{r}
st.cverror = rep(0,14)
for (i in 2:15){
  Auto$dis.cut <- cut(Auto$displacement, i)
  st.fit = glm( mpg~dis.cut, data = Auto)
  cverror[i-1] = cv.glm( Auto, st.fit, K = 10)$delta[1]
}
which.min(st.cverror)
```
```{r}
st.fit.1 = lm(mpg~cut(displacement,2), data = Auto)
st.pred = predict( st.fit.1, newdata=data.frame(displacement=dis.grid), se=T)
st.sebands = cbind( st.pred$fit + 2*st.pred$se.fit, st.pred$fit - 2*st.pred$se.fit)

plot(Auto$displacement, Auto$mpg, col="black")
lines( dis.grid, st.pred$fit, col="red", lwd=2)
matlines( dis.grid, st.sebands, col="red", lty=2, lwd=1)
```
Now, we use smoothing splines
```{r}
ss.fit = smooth.spline( Auto$displacement, Auto$mpg, cv=T)
ss.fit$df
```
```{r}
plot(Auto$displacement, Auto$mpg, col="darkgrey")
lines(ss.fit, col="red")
```
Finally,
```{r}
library(gam)
gam.8 = gam(mpg~poly(displacement, 11)+s(horsepower,4)+weight, data = Auto)

par(mfrow=c(1,3))
plot(gam.8, se=T)
```

## Question 9

```{r}
library(MASS)
```

### a
```{r}
pfit = lm(nox~poly(dis,3), data = Boston)
d.grid = seq(from = range(Boston$dis)[1], to = range(Boston$dis)[2], by = 0.1)
ppred = predict( pfit, newdata=list(dis=d.grid), se=T)
pse = cbind( ppred$fit+2*ppred$se.fit, ppred$fit-2*ppred$se.fit)

plot( Boston$dis, Boston$nox)
lines( d.grid, ppred$fit, col="red", lwd=2)
matlines( d.grid, pse, col="red", lwd=2, lty=2)
```
### b
```{r}
rss = rep(0,10)
for (i in 1:10){
  pl.fit = lm( nox~poly(dis,i), data = Boston)
  pl.pred = predict( pl.fit, Boston, type="response")
  rss[i] = sum((Boston$nox-pl.pred)^2)
}
rss
```
### c
```{r}
pcv = rep(0,10)
for (i in 1:10){
  p.fit = glm( nox~poly(dis,i), data = Boston)
  pcv[i] = cv.glm( Boston, p.fit, K=10)$delta[2]
}
pcv
```
### d
```{r}
rs.fit = lm(nox~bs(dis, df=4), data = Boston)
rs.pred = predict( rs.fit, newdata=list(dis=d.grid), se=T)
rs.se = cbind( rs.pred$fit+2*rs.pred$se.fit, rs.pred$fit-2*rs.pred$se.fit)

attr(bs(Boston$dis, df = 4), "knots")
```

```{r}
plot( Boston$dis, Boston$nox)
lines( d.grid, rs.pred$fit, col="red", lwd=2)
matlines( d.grid, rs.se, col="red", lwd=2, lty=2)
```
### e
```{r}
rs.rss = rep(0,10)
for (i in 4:13){
  rsfit = lm( nox~bs(dis,df=i), data = Boston)
  rspred = predict( rsfit, Boston, type="response")
  rs.rss[i-3] = sum((Boston$nox-rspred)^2)
}
rs.rss
```
### f
```{r warning=FALSE}
rs.cv = rep(0,10)
for (i in 4:13){
  rsfit = glm( nox~bs(dis,df=i), data = Boston)
  rs.cv[i-3] = cv.glm( Boston, rsfit, K=10)$delta[1]
}
rs.cv
```

## Question 10

### a
```{r}
train = sample(c(T,F), dim(College)[1], prob = c(8,2), replace = T)
test = (!train)
```
Performing forward stepwise selection,
```{r}
library(leaps)
fwd = regsubsets( Outstate~., data = College, subset=train, nvmax=17, method="forward")
summary(fwd)
```
```{r}
test.mat = model.matrix(Outstate~., data = College[test,])
val.err = rep(0, 17)
for (i in 1:17){
  coefi = coef(fwd, id=i)
  pred = test.mat[,names(coefi)]%*%coefi
  val.err[i] = mean((College$Outstate[test] - pred)^2)
}
which.min(val.err)
```
```{r}
coef( fwd, id=13)
```
### b
```{r}
col.gam = gam( Outstate~Private+s(Apps,2)+poly(Accept,3)+Top10perc+F.Undergrad+Room.Board+Personal+PhD+Terminal+S.F.Ratio+perc.alumni+Expend+Grad.Rate, data = College, subset = train)

par(mfrow=c(4,4))
plot(col.gam, se=T)
```
There are many possibilties here
### c
```{r}
col.pred = predict( col.gam, College[test,])
col.rss = sum((College$Outstate[test] - col.pred)^2)
col.tss = sum((College$Outstate[test] - mean(College$Outstate[test]))^2)

(col.tss-col.rss)/col.tss
```
### d
This will can be further investigated after we do (b) part of this question

## Question 11

### a
```{r}
set.seed(5)
y = rnorm(100)
x1 = rnorm(100)
x2 = rnorm(100)
```
### b
```{r}
b1 = 5
```
### c
```{r}
a = y-b1*x1
b2 = lm(a~x2)$coef[2]
```
### d
```{r}
a = y-b2*x2
b1 = lm(a~x1)$coef[2]
```
### e
```{r}
beta0.1 = rep(NA, 1000)
beta0.2 = rep(NA, 1000)
beta1 = rep(NA, 1001)
beta2 = rep(NA, 1000)

beta1[1] = 0.5
for (i in 1:1000){
  es2 = y-beta1[i]*x1
  beta0.2[i] = lm(es2~x2)$coef[1]
  beta2[i] = lm(es2~x2)$coef[2]
  
  es1 = y-beta2[i]*x2
  beta0.1[i] = lm(es1~x1)$coef[1]
  beta1[i+1] = lm(es1~x1)$coef[2]
}
```
Plotting the same,
```{r}
plot(1:1000, beta0.1, col="grey", ylim=c(0,0.2))
lines(1:1000, beta0.2, col="black")
lines(1:1000, beta1[-1], col="red")
lines(1:1000, beta2, col="blue")
legend("topleft", c("beta0.1", "beta0.2", "beta1", "beta2"), lty = 1, col = c("grey", "black", "red", 
    "blue"))
```
### f
```{r}
mlr = lm(y~x1+x2)

plot(1:1000, beta0.1, col="grey", ylim=c(0,0.2), lwd=3)
lines(1:1000, beta0.2, col="black", lwd=3)
lines(1:1000, beta1[-1], col="red", lwd=3)
lines(1:1000, beta2, col="blue", lwd=3)
lines(1:1000, rep(mlr$coe[1], 1000), col="yellow", lty=2)
lines(1:1000, rep(mlr$coe[2], 1000), col="yellow", lty=2)
lines(1:1000, rep(mlr$coe[3], 1000), col="yellow", lty=2)
```
Clearly, they all overlap each other