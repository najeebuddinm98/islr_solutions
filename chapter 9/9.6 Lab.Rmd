---
title: "9.6 Lab:Support Vector Machines"
output:
  html_document:
    df_print: paged
---

# 9.6.1 Support Vector Classifier

```{r}
set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
y = c(rep(-1,10), rep(1,10))
x[y==1,] = x[y==1,]+1
plot(x, col=(3-y))
```
The above data is not linearly separable perfectly.
```{r}
dat = data.frame(x=x, y=as.factor(y))
library(e1071)
svmfit = svm( y~., data=dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit, dat)
```
The support vectors are
```{r}
svmfit$index
```
```{r}
summary(svmfit)
```
```{r}
svmfit.less = svm(y~., data = dat, cost=0.1, kernel="linear", scale = FALSE)
plot(svmfit.less, dat)
```
```{r}
svmfit.less$index
```
Smaller the cost value, wider is the margin resulting in more support vectors.
Now, we look at cross-validation to get best cost parameter value
```{r}
set.seed(1)
tune.out = tune(svm, y~., data = dat, kernel="linear", ranges=list(cost=c(0.001,0.001,0.1,1,5,10,100)))
summary(tune.out)
```
cost=0.1 gives the lowest cv error.
```{r}
bestmod = tune.out$best.model
summary(bestmod)
```
Generating our test dataset,
```{r}
xtest = matrix(rnorm(20*2), ncol = 2)
ytest = sample(c(-1,1), 20, replace = TRUE)
xtest[ytest==1,] = xtest[ytest==1,] + 1
testdat = data.frame(x=xtest, y=as.factor(ytest))

ypred = predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
```
```{r}
svmfit2 = svm( y~., data = dat, kernel="linear", scale = F, cost = 0.01)
ypred2 = predict(svmfit2, testdat)
table(predict=ypred2, truth=testdat$y)
```
Now, we take observations that are barely linearly separable.
```{r}
x[y==1,] = x[y==1,] + 1
plot(x, col=(y+5)/2, pch=19)
```
```{r}
dat3 = data.frame(x=x, y=as.factor(y))
svmfit3 = svm( y~., data = dat3, kernel="linear", cost=1e5, scale=F)
summary(svmfit3)
```
```{r}
plot(svmfit3, dat3)
```
```{r}
svmfit4 = svm( y~., data = dat3, kernel="linear", cost=1, scale=F)
summary(svmfit4)
```
```{r}
plot(svmfit4, dat3)
```

# 9.6.2 Support Vector Machine

```{r}
set.seed(1)
x = matrix(rnorm(200*2), ncol=2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2
y = c(rep(1,150), rep(2,50))
dat5 = data.frame(x=x, y=as.factor(y))
plot(x, col=y)
```
```{r}
train = sample(200,100)
svmfit.5 = svm( y~., data = dat5[train,], kernel="radial", gamma=1, cost=1)
plot(svmfit.5, dat5[train,])
```
```{r}
summary(svmfit.5)
```
```{r}
svmfit.6 = svm( y~., data = dat5[train,], kernel="radial", gamma=1, cost=1e5)
plot(svmfit.6, dat5[train,])
```
Performing CV,
```{r}
set.seed(1)
tune.out2 = tune(svm, y~., data = dat5[train,], kernel="radial", 
                 ranges = list(cost=c(0.1, 1, 10, 100, 1000), 
                               gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out2)
```
```{r}
table(true=dat5[-train, "y"], pred=predict(tune.out2$best.model, newdata = dat5[-train,]))
```

# 9.6.3 ROC Curves

We first write a fucntion to plot the ROC curve
```{r}
library(ROCR)
rocplot=function(pred, truth, ...){
   predob = prediction(pred, truth, label.ordering = c(2,1))
   perf = performance(predob, "tpr", "fpr")
   plot(perf,...)}

svmfit.opt=svm(y~., data=dat5[train,], kernel="radial",gamma=2, cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat5[train,],decision.values=TRUE))$decision.values

par(mfrow=c(1,2))
rocplot(fitted,dat5[train,"y"],main="Training Data")

svmfit.flex=svm(y~., data=dat5[train,], kernel="radial",gamma=50, cost=1, decision.values=T)
fitted=attributes(predict(svmfit.flex,dat5[train,],decision.values=T))$decision.values
rocplot(fitted,dat5[train,"y"],add=T,col="red")

fitted=attributes(predict(svmfit.opt,dat5[-train,],decision.values=T))$decision.values
rocplot(fitted,dat5[-train,"y"],main="Test Data")

fitted=attributes(predict(svmfit.flex,dat5[-train,],decision.values=T))$decision.values
rocplot(fitted,dat5[-train,"y"],add=T,col="red")
```
The reason we need to mention `label.ordering' parameter is answered below
https://stackoverflow.com/questions/54148554/roc-curve-for-perfect-labeling-is-produced-upside-down-by-package-rocr

# 9.6.4 SVM with Multiple Classes

```{r}
set.seed(1)
x = rbind(x, matrix(rnorm(50*2), ncol = 2))
y = c(y, rep(0,50))
x[y==0,] = x[y==0,] + 2
dat.mul = data.frame(x=x, y=as.factor(y))
plot(x, col=(y+1))
```
```{r}
svmfit.mul = svm( y~., data = dat.mul, kernel="radial", cost=10, gamma=1)
plot(svmfit.mul, dat.mul)
```
Pretty bad performnace for class 0, but good enough for 1 and 2.

# 9.6.5 Application to Gene Expression Data

```{r}
library(ISLR)
names(Khan)
```
```{r}
print(dim(Khan$xtrain))
print(dim(Khan$xtest))
print(length(Khan$ytrain))
print(length(Khan$ytest))
```
```{r}
table(Khan$ytrain)
```
```{r}
table(Khan$ytest)
```
```{r}
dat.tr = data.frame( x=Khan$xtrain, y=as.factor(Khan$ytrain))
out = svm( y~., data = dat.tr, kernel="linear", cost=10)
summary(out)
```
```{r}
table(out$fitted, dat.tr$y)
```
High dimensional data, so easy to find a separating hyperplane.
Now, for our test dataset
```{r}
dat.te = data.frame( x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te = predict(out, newdata = dat.te)
table(pred.te, dat.te$y)
```
```{r}
(2)/(3+6+4+5+2)
```
We have an accuracy of 0.9
